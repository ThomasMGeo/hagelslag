#!/usr/bin/env python
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import ElasticNetCV
from datetime import datetime


num_procs = 10
model_names = ["Random Forest", "Gradient Boosting"]
model_objs = [RandomForestClassifier(n_estimators=100,
                                     min_samples_split=15,
                                     max_features="sqrt",
                                     class_weight="auto",
                                     n_jobs=num_procs),
              GradientBoostingClassifier(n_estimators=100,
                                         learning_rate=0.05,
                                         max_features="sqrt",
                                         min_samples_split=15),
              ]
# Models for predicting a gamma size pdf
dist_model_names = ["Random Forest", "Gradient Boosting", "Elastic Net"]
dist_model_objs = [RandomForestRegressor(n_estimators=100,
                                         min_samples_split=10,
                                         max_features="sqrt",
                                         n_jobs=num_procs),
                   GradientBoostingRegressor(n_estimators=100,
                                             learning_rate=0.05,
                                             max_features="sqrt",
                                             min_samples_split=10),
                   ElasticNetCV(l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99], n_jobs=num_procs)]
variables = ["UP_HELI_MAX", "GRPL_MAX", "W_UP_MAX", "W_DN_MAX", "WSPD10MAX", "UP_HELI_MIN"]
var_stats = ["mean", "max", "min", "std", "mean_dt", "max_dt"]
# variables describing the shape of the object have been added
shape_variables = ["area", "eccentricity", "major_axis_length", "minor_axis_length", "orientation",
                   "extent"] + ["weighted_moments_hu_{0:d}".format(h) for h in range(7)]
input_columns = ["Forecast_Hour", "Valid_Hour_UTC", "Duration_Step", "Duration_Total", "Centroid_Lon", "Centroid_Lat"]
for var in variables:
    for stat in var_stats:
        input_columns.append(var + "_" + stat)
input_columns.extend(shape_variables)
# change the scratch path to your directory
scratch_path = "/glade/scratch/dgagne/"
ensemble_members = ["mem{0:d}".format(m) for m in range(1, 11)]
config = dict(ensemble_name="NCAR", # name of the ensemble; should match your data files
              ensemble_members=ensemble_members, # ensemble member list
              num_procs=num_procs, # number of processors; used in neighborhood probability generation
              start_dates={"train": datetime(2015, 5, 1), "forecast": datetime(2015, 6, 1)}, # beginning run dates
              end_dates={"train": datetime(2015, 5, 31), "forecast": datetime(2015, 6, 30)}, # ending run dates
              start_hour=12, # first forecast hour
              end_hour=36, # last forecast hour
              map_filename="/glade/u/home/dgagne/hagelslag/mapfiles/ncar_ensemble_map_2015.txt",
              train_data_path=scratch_path + "track_data_ncar_2015_csv/",
              forecast_data_path=scratch_path + "track_data_ncar_2015_csv/",
              member_files={"train": scratch_path + "member_info_ncar_2015.csv",
                            "forecast": scratch_path + "member_info_ncar_2015.csv"},
              data_format="csv",
              group_col="Microphysics",
              condition_model_names=model_names,
              condition_model_objs=model_objs,
              condition_input_columns=input_columns,
              condition_output_column="Hail_Size",
              output_threshold=5,
              size_model_names=model_names,
              size_model_objs=model_objs,
              size_input_columns=input_columns,
              size_output_column="Hail_Size",
              size_range_params=(5, 100, 5),
              # Size distribution models predict the scale parameter of the gamma distribution
              # Then a linear regression is fitted between the log of the predicted scale parameter
              # and the log of the observed shape parameter.
              size_distribution_model_names=dist_model_names,
              size_distribution_model_objs=dist_model_objs,
              size_distribution_input_columns=input_columns,
              size_distribution_shape_column="Shape",
              size_distribution_scale_column="Scale",
              track_model_names=model_names,
              track_model_objs=model_objs,
              track_input_columns=input_columns,
              track_output_columns={"translation-x": "Translation_Error_X",
                                    "translation-y": "Translation_Error_Y",
                                    "start-time": "Start_Time_Error"},
              track_output_ranges={"translation-x": (-240000, 240000, 24000),
                                   "translation-y": (-240000, 240000, 24000),
                                   "start-time": (-6, 6, 1),
                                   },
              load_models=True,
              model_path=scratch_path + "track_models_ncar_2015/",
              metadata_columns=["Track_ID", "Step_ID"],
              data_json_path=scratch_path + "track_data_ncar_2015_json/",
              forecast_json_path=scratch_path + "track_forecasts_ncar_2015_json/",
              copula_file=scratch_path + "track_copulas_ncar_2015.pkl",
              num_track_samples=1000,
              sampler_thresholds=[25, 50],
              sampler_out_path=scratch_path + "track_samples_ncar_2015/",
              # Variables used for generating neighborhood probabilities
              ensemble_variables=["HAIL_MAX2D", "HAIL_MAXK1", "UP_HELI_MAX"],
              # Neighborhood probability thresholds
              ensemble_variable_thresholds={"UP_HELI_MAX": [75],
                                            "HAIL_MAXK1":[25, 50],
                                            "HAIL_MAX2D": [25, 50]},
              # Setting ml_grid_method as gamma will use the new models to generate the neighborhood probabilities
              # To use the bin models, set ml_grid_method="mean"
              ml_grid_method="gamma",
              neighbor_radius=[14],
              neighbor_sigma=[5, 40],
              # Dimensions of the NCAR ensemble grid (y, x)
              grid_shape=(985, 1580),
              # Output path of neighborhood probability netCDF files.
              ensemble_consensus_path=scratch_path + "ensemble_consensus_ncar_2015/",
              # Path to ensemble data files
              ensemble_data_path="/glade/scratch/sobash/RT2015/",
              # If true, each model time step is stored in a separate netCDF file. False if they are grouped together.
              single_step=False,
              )
