import numpy as np
import pandas as pd
import cPickle
import json
import os
from copy import copy
from glob import glob


class TrackModeler(object):
    """
    TrackModeler is designed to load and process data generated by TrackProcessing and then use that data to fit
    machine learning models to predict whether or not hail will occur, hail size, and translation errors in time and
    space.
    """
    def __init__(self,
                 ensemble_name,
                 train_data_path,
                 forecast_data_path,
                 member_files,
                 group_col="Microphysics"):
        self.train_data_path = train_data_path
        self.forecast_data_path = forecast_data_path
        self.ensemble_name = ensemble_name
        self.member_files = member_files
        self.data = {"train": {}, "forecast": {}}
        self.condition_models = {}
        self.size_models = {}
        self.track_models = {"translation-x": {},
                             "translation-y": {},
                             "start-time": {}}
        self.group_col = group_col
        return

    def load_data(self, mode="train", format="csv"):
        """
        Load data from flat data files containing total track information and information about each timestep.
        The two sets are combined using merge operations on the Track IDs. Additional member information is gathered
        from the appropriate member file.

        :param mode: "train" or "forecast"
        :param format: file format being used. Default is "csv"
        """
        if mode in self.data.keys():
            total_track_files = sorted(glob(getattr(self, mode + "_data_path") + \
                                            "*total_" + self.ensemble_name + "*." + format))
            step_track_files = sorted(glob(getattr(self, mode + "_data_path") + \
                                           "*step_" + self.ensemble_name + "*." + format))
            self.data[mode]["total"] = pd.concat(map(pd.read_csv, total_track_files),
                                                 ignore_index=True)
            self.data[mode]["step"] = pd.concat(map(pd.read_csv, step_track_files),
                                                ignore_index=True)
            self.data[mode]["member"] = pd.read_csv(self.member_files[mode])
            self.data[mode]["combo"] = pd.merge(self.data[mode]["step"],
                                                self.data[mode]["total"],
                                                on="Track_ID",
                                                suffixes=("_Step", "_Total"))
            self.data[mode]["combo"] = pd.merge(self.data[mode]["combo"],
                                                self.data[mode]["member"],
                                                on="Ensemble_Member")
            self.data[mode]["total_group"] = pd.merge(self.data[mode]["total"],
                                                      self.data[mode]["member"],
                                                      on="Ensemble_Member")

    def calc_copulas(self,
                     output_file,
                     model_names=("start-time", "translation-x", "translation-y"),
                     label_columns=("Start_Time_Error", "Translation_Error_X", "Translation_Error_Y")):
        """
        Calculate a copula multivariate normal distribution from the training data for each group of ensemble members.
        Distributions are written to a pickle file for later use.

        :param output_file: pickle file copula distributions are written to.
        :param model_names: Names of the output models being linked with the copula
        :param label_columns: Columns in the training data files that correspond to the copulas
        :return:
        """
        if len(self.data['train']) == 0:
            self.load_data()
        groups = self.data["train"]["member"][self.group_col].unique()
        copulas = {}
        for group in groups:
            print group
            group_data = self.data["train"]["total_group"].loc[
                self.data["train"]["total_group"][self.group_col] == group]
            group_data.dropna(inplace=True)
            group_data.reset_index(drop=True, inplace=True)
            copulas[group] = {}
            copulas[group]["mean"] = group_data[label_columns].mean(axis=0).values
            copulas[group]["cov"] = np.cov(group_data[label_columns].values.T)
            copulas[group]["model_names"] = model_names
            del group_data
        cPickle.dump(copulas, open(output_file, "w"), cPickle.HIGHEST_PROTOCOL)

    def fit_condition_models(self, model_names,
                             model_objs,
                             input_columns,
                             output_column="Hail_Size",
                             output_threshold=0.0):
        """
        Fit machine learning models to predict whether or not hail will occur.

        :param model_names: List of strings with the names for the particular machine learning models
        :param model_objs: scikit-learn model objects that have already been initialized with tuning parameters.
        :param input_columns: List of input columns for the models being trained.
        :param output_column: Column being used as the predictive label.
        :param output_threshold: Size threshold used to determine if hail is occurring or not.
        :return:
        """
        print "Fitting condition models"
        groups = self.data["train"]["member"][self.group_col].unique()
        for group in groups:
            print group
            group_data = self.data["train"]["combo"].loc[self.data["train"]["combo"][self.group_col] == group]
            output_data = np.where(group_data[output_column] > output_threshold, 1, 0)
            self.condition_models[group] = {}
            for m, model_name in enumerate(model_names):
                print model_name
                self.condition_models[group][model_name] = copy(model_objs[m])
                self.condition_models[group][model_name].fit(group_data[input_columns], output_data)

    def predict_condition_models(self, model_names,
                                 input_columns,
                                 metadata_cols,
                                 data_mode="forecast",
                                 ):
        """
        Apply condition models to forecast data.

        :param model_names:
        :param input_columns:
        :param metadata_cols:
        :param data_mode:
        :return:
        """
        groups = self.condition_models.keys()
        predictions = {}
        for group in groups:
            group_data = self.data[data_mode]["combo"].loc[self.data[data_mode]["combo"][self.group_col] == group]
            if group_data.shape[0] > 0:
                predictions[group] = group_data[metadata_cols]
                for m, model_name in enumerate(model_names):
                    predictions[group][model_name] = self.condition_models[group][model_name].predict_proba(
                        group_data[input_columns])[:, 1]
        return predictions

    def fit_size_models(self, model_names,
                        model_objs,
                        input_columns,
                        output_column="Hail_Size",
                        output_start=5,
                        output_step=5,
                        output_stop=100):
        """
        Fit size models to produce discrete pdfs of forecast hail sizes.

        :param model_names:
        :param model_objs:
        :param input_columns:
        :param output_column:
        :param output_start:
        :param output_step:
        :param output_stop:
        :return:
        """
        print "Fitting size models"
        groups = self.data["train"]["member"][self.group_col].unique()
        output_start = int(output_start)
        output_step = int(output_step)
        output_stop = int(output_stop)
        for group in groups:
            print group
            group_data = self.data["train"]["combo"].loc[self.data["train"]["combo"][self.group_col] == group]
            group_data.dropna(inplace=True)
            group_data = group_data[group_data[output_column] >= output_start]
            output_data = group_data[output_column].values.astype(int)
            output_data[output_data > output_stop] = output_stop
            discrete_data = ((output_data - output_start) // output_step) * output_step + output_start
            self.size_models[group] = {}
            self.size_models[group]["outputvalues"] = np.arange(output_start, output_stop + output_step, output_step,
                                                                dtype=int)
            print "Discrete values", np.unique(discrete_data)
            print "Output", self.size_models[group]["outputvalues"]
            for m, model_name in enumerate(model_names):
                print model_name
                self.size_models[group][model_name] = copy(model_objs[m])
                self.size_models[group][model_name].fit(group_data[input_columns], discrete_data)

    def predict_size_models(self, model_names,
                            input_columns,
                            metadata_cols,
                            data_mode="forecast"):
        """
        Apply size models to forecast data.

        :param model_names:
        :param input_columns:
        :param metadata_cols:
        :param data_mode:
        :return:
        """
        groups = self.size_models.keys()
        predictions = {}
        for group in groups:
            group_data = self.data[data_mode]["combo"].loc[self.data[data_mode]["combo"][self.group_col] == group]
            if group_data.shape[0] > 0:
                predictions[group] = {}
                output_values = self.size_models[group]["outputvalues"].astype(int)
                for m, model_name in enumerate(model_names):
                    print model_name
                    pred_col_names = [model_name.replace(" ", "-") + "_{0:02d}".format(p) for p in output_values]
                    predictions[group][model_name] = group_data[metadata_cols]
                    pred_vals = self.size_models[group][model_name].predict_proba(group_data[input_columns])
                    pred_classes = self.size_models[group][model_name].classes_
                    pred_pdf = np.zeros((pred_vals.shape[0], output_values.size))
                    for pcv, pc in enumerate(pred_classes):
                        idx = np.where(output_values == pc)[0][0]
                        pred_pdf[:, idx] = pred_vals[:, pcv]
                    print "Pred classes", pred_classes
                    print "Output values", output_values
                    print "Pred Vals Sum", pred_vals.sum(axis=1).min(), pred_vals.sum(axis=1).max(), pred_vals.shape
                    print "Pred PDF Sum", pred_pdf.sum(axis=1).min(), pred_pdf.sum(axis=1).max(), pred_pdf.shape

                    for pcn, pred_col_name in enumerate(pred_col_names):
                        predictions[group][model_name][pred_col_name] = pred_pdf[:, pcn]
        return predictions

    def fit_track_models(self,
                         model_names,
                         model_objs,
                         input_columns,
                         output_columns,
                         output_ranges,
                         ):
        """
        Fit machine learning models to predict track error offsets.

        :param model_names:
        :param model_objs:
        :param input_columns:
        :param output_columns:
        :param output_ranges:
        :return:
        """
        print "Fitting track models"
        groups = self.data["train"]["member"][self.group_col].unique()
        for group in groups:
            print group
            group_data = self.data["train"]["combo"].loc[self.data["train"]["combo"][self.group_col] == group]
            group_data = group_data.dropna()
            group_data = group_data.loc[group_data["Duration_Step"] == 1]
            for model_type, model_dict in self.track_models.iteritems():
                print model_type
                model_dict[group] = {}
                output_data = group_data[output_columns[model_type]].values.astype(int)
                output_data[output_data < output_ranges[model_type][0]] = output_ranges[model_type][0]
                output_data[output_data > output_ranges[model_type][1]] = output_ranges[model_type][1]
                discrete_data = (output_data - output_ranges[model_type][0]) // output_ranges[model_type][2] * \
                    output_ranges[model_type][2] + output_ranges[model_type][0]
                model_dict[group]["outputvalues"] = np.arange(output_ranges[model_type][0],
                                                              output_ranges[model_type][1] +
                                                              output_ranges[model_type][2],
                                                              output_ranges[model_type][2])
                print "Discrete values", np.unique(discrete_data)
                for m, model_name in enumerate(model_names):
                    print model_name
                    model_dict[group][model_name] = copy(model_objs[m])
                    model_dict[group][model_name].fit(group_data[input_columns], discrete_data)

    def predict_track_models(self, model_names,
                             input_columns,
                             metadata_cols,
                             data_mode="forecast",
                             ):
        """
        Predict track offsets on forecast data.

        :param model_names:
        :param input_columns:
        :param metadata_cols:
        :param data_mode:
        :return:
        """
        predictions = {}
        for model_type, track_model_set in self.track_models.iteritems():
            predictions[model_type] = {}
            groups = track_model_set.keys()
            for group in groups:
                group_data = self.data[data_mode]["combo"].loc[self.data[data_mode]["combo"][self.group_col] == group]
                if group_data.shape[0] > 0:
                    predictions[model_type][group] = {}
                    output_values = track_model_set[group]["outputvalues"].astype(int)
                    for m, model_name in enumerate(model_names):
                        pred_col_names = [model_name.replace(" ", "-") + "_{0:02d}".format(p) for p in output_values]
                        predictions[model_type][group][model_name] = group_data[metadata_cols]
                        pred_vals = track_model_set[group][model_name].predict_proba(group_data[input_columns])
                        pred_classes = track_model_set[group][model_name].classes_
                        pred_pdf = np.zeros((pred_vals.shape[0], output_values.size))
                        for pcv, pc in enumerate(pred_classes):
                            idx = np.where(pc == output_values)[0][0]
                            pred_pdf[:, idx] = pred_vals[:, pcv]
                        for pcn, pred_col_name in enumerate(pred_col_names):
                            predictions[model_type][group][model_name][pred_col_name] = pred_pdf[:, pcn]

                        print "Pred classes", pred_classes
                        print "Output values", output_values
                        print "Pred Vals Sum", pred_vals.sum(axis=1).min(), pred_vals.sum(axis=1).max(), pred_vals.shape
                        print "Pred PDF Sum", pred_pdf.sum(axis=1).min(), pred_pdf.sum(axis=1).max(), pred_pdf.shape
        return predictions

    def save_models(self, model_path):
        """
        Save machine learning models to pickle files.

        :param model_path:
        :return:
        """
        for group, condition_model_set in self.condition_models.iteritems():
            for model_name, model_obj in condition_model_set.iteritems():
                out_filename = model_path + \
                               "{0}_{1}_condition.pkl".format(group,
                                                              model_name.replace(" ", "-"))
                with open(out_filename, "w") as pickle_file:
                    cPickle.dump(model_obj,
                                 pickle_file,
                                 cPickle.HIGHEST_PROTOCOL)
        for group, size_model_set in self.size_models.iteritems():
            for model_name, model_obj in size_model_set.iteritems():
                out_filename = model_path + \
                               "{0}_{1}_size.pkl".format(group,
                                                         model_name.replace(" ", "-"))
                with open(out_filename, "w") as pickle_file:
                    cPickle.dump(model_obj,
                                 pickle_file,
                                 cPickle.HIGHEST_PROTOCOL)
        for model_type, track_type_models in self.track_models.iteritems():
            for group, track_model_set in track_type_models.iteritems():
                for model_name, model_obj in track_model_set.iteritems():
                    out_filename = model_path + \
                                   "{0}_{1}_{2}_track.pkl".format(group,
                                                                  model_name.replace(" ", "-"),
                                                                  model_type)
                    with open(out_filename, "w") as pickle_file:
                        cPickle.dump(model_obj,
                                     pickle_file,
                                     cPickle.HIGHEST_PROTOCOL)

        return

    def load_models(self, model_path):
        """
        Load models from pickle files.

        :param model_path:
        :return:
        """
        condition_model_files = sorted(glob(model_path + "*_condition.pkl"))
        if len(condition_model_files) > 0:
            for condition_model_file in condition_model_files:
                model_comps = condition_model_file.split("/")[-1][:-4].split("_")
                if model_comps[0] not in self.condition_models.keys():
                    self.condition_models[model_comps[0]] = {}
                model_name = model_comps[1].replace("-", " ")
                with open(condition_model_file) as cmf:
                    self.condition_models[model_comps[0]][model_name] = cPickle.load(cmf)
        size_model_files = sorted(glob(model_path + "*_size.pkl"))
        if len(size_model_files) > 0:
            for size_model_file in size_model_files:
                model_comps = size_model_file.split("/")[-1][:-4].split("_")
                if model_comps[0] not in self.size_models.keys():
                    self.size_models[model_comps[0]] = {}
                model_name = model_comps[1].replace("-", " ")
                with open(size_model_file) as smf:
                    self.size_models[model_comps[0]][model_name] = cPickle.load(smf)
        track_model_files = sorted(glob(model_path + "*_track.pkl"))
        if len(track_model_files) > 0:
            for track_model_file in track_model_files:
                model_comps = track_model_file.split("/")[-1][:-4].split("_")
                group = model_comps[0]
                model_name = model_comps[1].replace("-", " ")
                model_type = model_comps[2]
                if model_type not in self.track_models.keys():
                    self.track_models[model_type] = {}
                if group not in self.track_models[model_type].keys():
                    self.track_models[model_type][group] = {}
                with open(track_model_file) as tmf:
                    self.track_models[model_type][group][model_name] = cPickle.load(tmf)

    def output_forecasts_json(self, forecasts,
                              condition_model_names,
                              size_model_names,
                              track_model_names,
                              json_data_path,
                              out_path):
        """
        Output forecast values to geoJSON file format.

        :param forecasts:
        :param condition_model_names:
        :param size_model_names:
        :param track_model_names:
        :param json_data_path:
        :param out_path:
        :return:
        """
        total_tracks = self.data["forecast"]["total"]
        for r in np.arange(total_tracks.shape[0]):
            track_id = total_tracks.loc[r, "Track_ID"]
            print track_id
            track_num = track_id.split("_")[-1]
            ensemble_name = total_tracks.loc[r, "Ensemble_Name"]
            member = total_tracks.loc[r, "Ensemble_Member"]
            group = self.data["forecast"]["member"].loc[self.data["forecast"]["member"]["Ensemble_Member"] == member,
                                                        self.group_col].values[0]
            run_date = track_id.split("_")[3][:8]
            step_forecasts = {}
            for mlmodel in condition_model_names:
                step_forecasts["condition_" + mlmodel.replace(" ", "-")] = forecasts["condition"][group].loc[
                    forecasts["condition"][group]["Track_ID"] == track_id, mlmodel]
            for mlmodel in size_model_names:
                step_forecasts["size_" + mlmodel.replace(" ", "-")] = forecasts["size"][group][mlmodel].loc[
                    forecasts["size"][group][mlmodel]["Track_ID"] == track_id]
            for model_type in forecasts["track"].keys():
                for mlmodel in track_model_names:
                    mframe = forecasts["track"][model_type][group][mlmodel]
                    step_forecasts[model_type + "_" + mlmodel.replace(" ", "-")] = mframe.loc[
                        mframe["Track_ID"] == track_id]
            json_file_name = "{0}_{1}_{2}_model_track_{3}.json".format(ensemble_name,
                                                                       run_date,
                                                                       member,
                                                                       track_num)
            full_json_path = json_data_path + "/".join([run_date, member]) + "/" + json_file_name
            with open(full_json_path) as json_file_obj:
                try:
                    track_obj = json.load(json_file_obj)
                except:
                    print full_json_path + " not found"
                    continue
            for f, feature in enumerate(track_obj['features']):
                del feature['properties']['attributes']
                del feature['properties']['timesteps']
                for model_name, fdata in step_forecasts.iteritems():
                    ml_model_name = model_name.split("_")[1]
                    if "condition" in model_name:
                        feature['properties'][model_name] = fdata.values[f]
                    else:
                        predcols = []
                        for col in fdata.columns:
                            if ml_model_name in col:
                                predcols.append(col)
                        feature['properties'][model_name] = fdata.loc[:, predcols].values[f].tolist()
            full_path = []
            for part in [run_date, member]:
                full_path.append(part)
                if not os.access(out_path + "/".join(full_path), os.R_OK):
                    try:
                        os.mkdir(out_path + "/".join(full_path))
                    except OSError:
                        print "directory already created"
            out_json_filename = out_path + "/".join(full_path) + "/" + json_file_name
            with open(out_json_filename, "w") as out_json_obj:
                json.dump(track_obj, out_json_obj, indent=1, sort_keys=True)
        return
