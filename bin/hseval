#!/usr/bin/env python
import argparse
import traceback
from hagelslag.util.Config import Config
import pandas as pd
import numpy as np
from multiprocessing import Pool
from hagelslag.evaluation.ObjectEvaluator import ObjectEvaluator
from hagelslag.evaluation.GridEvaluator import GridEvaluator


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("config", help="Config file")
    parser.add_argument("-o", "--obj", action="store_true", help="Perform object-based evaluation.")
    parser.add_argument("-g", "--grid", action="store_true", help="Perform grid-based evaluation.")
    parser.add_argument("-p", "--proc", type=int, default=1, help="Number of processors.")
    args = parser.parse_args()
    required = ["ensemble_name", "ensemble_members", "start_date", "end_date", "start_hour", "end_hour", "window_size",
                "time_skip", "model_names", "model_types", "size_thresholds", "forecast_json_path",
                "track_data_csv_path", "forecast_sample_path", "mrms_path", "mrms_variable", "obs_mask",
                "mask_variable", "forecast_thresholds", "forecast_bins", "out_path", "obj_scores_file",
                "grid_scores_file"]
    config = Config(args.config, required_attributes=required)
    num_procs = args.proc
    if args.obj:
        all_scores = evaluate_objects(config, num_procs)
        #all_scores.to_csv(config.out_path + config.obj_scores_file, index_label="Index")
    if args.grid:
        grid_scores = evaluate_grids(config, num_procs)
        #grid_scores.to_csv(config.out_path + config.grid_scores_file, index_label="Index")
    return


def evaluate_objects(config, num_procs):
    pool = Pool(num_procs)
    run_dates = pd.DatetimeIndex(start=config.start_date,
                                 end=config.end_date, freq="1D")
    ensemble_members = config.ensemble_members
    score_columns = ["Run_Date", "Ensemble_Name", "Ensemble_Member", "Model_Name", "Model_Type", "CRPS"]
    score_columns += ["ROC_{0:d}".format(int(t)) for t in config.object_thresholds]
    score_columns += ["Rel_{0:d}".format(int(t)) for t in config.object_thresholds]
    score_list = []

    def append_scores(score_set):
        score_list.append(score_set)
        if len(score_list) == 1:
            score_set.to_csv(config.out_path + config.obj_scores_file, index_label="Index")
        else:
            score_set.to_csv(config.out_path + config.obj_scores_file, mode="a", index_label="Index", header=False)

    for run_date in run_dates:
        for member in ensemble_members:
            pool.apply_async(evaluate_object_run, (run_date, member, config, score_columns),
                             callback=append_scores)
    pool.close()
    pool.join()
    return pd.concat(score_list)


def evaluate_object_run(run_date, ensemble_member, config, score_columns):
    try:
        print("Starting {0} {1}".format(run_date, ensemble_member))
        object_eval = ObjectEvaluator(run_date, config.ensemble_name, ensemble_member, config.model_names,
                                    config.model_types, config.forecast_bins, config.forecast_json_path,
                                    config.track_data_csv_path)
        print("Loading forecasts")
        object_eval.load_forecasts()
        print("Loading obs")
        object_eval.load_obs()
        print("Merging obs")
        object_eval.merge_obs()
        scores = pd.DataFrame(columns=score_columns)
        query_start = "(Hail_Size>0)"
        forecast_hours = np.arange(config.start_hour, config.end_hour + 1)
        for model_type in config.model_types:
            for model_name in config.model_names:
                for forecast_hour in forecast_hours:
                    print("{0} {1} {2:d}".format(model_type, model_name, forecast_hour))
                    query = query_start + " & " + "(Forecast_Hour == {0:d})".format(forecast_hour)
                    crps = object_eval.crps(model_type, model_name, query=query)
                    rocs = []
                    rels = []
                    for threshold in config.object_thresholds:
                        rocs.append(object_eval.roc(model_type, model_name, threshold,
                                                    config.forecast_thresholds, query=query))
                        rels.append(object_eval.reliability(model_type, model_name, threshold,
                                                            config.forecast_thresholds, query=query))
                    row = [run_date, config.ensemble_name, ensemble_member, model_name, model_type, forecast_hour, crps]
                    row += rocs + rels
                    index = "{0}_{1}_{2}_{3}_{4}".format(run_date.strftime("%Y%m%d"), config.ensemble_name,
                                                        ensemble_member, model_name, model_type)
                    scores.loc[index] = row
        return scores
    except Exception as e:
        print(traceback.format_exc())
        raise e


def evaluate_grids(config, num_procs):
    pool = Pool(num_procs)
    run_dates = pd.DatetimeIndex(start=config.start_date,
                                 end=config.end_date, freq="1D")
    ensemble_members = config.ensemble_members
    score_columns = ["Run_Date", "Ensemble_Name", "Ensemble_Member", "Model_Name", "Size_Threshold", "Window_Size",
                     "Window_Start", "Window_End", "ROC", "Reliability"]
    score_list = []

    def append_scores(score_set):
        score_list.append(score_set)
        if len(score_list) == 1:
            score_set.to_csv(config.out_path + config.grid_scores_file, index_label="Index")
        else:
            score_set.to_csv(config.out_path + config.grid_scores_file, mode="a", index_label="Index", header=False)
    for window_size in config.window_sizes:
        for run_date in run_dates:
            for member in ensemble_members:
                pool.apply_async(evaluate_grid_run, (run_date, member, window_size, config,
                                                     score_columns),
                                 callback=append_scores)
    pool.close()
    pool.join()
    all_scores = pd.concat(score_list)
    return all_scores


def evaluate_grid_run(run_date, member, window_size, config, score_columns):
    """
    Calculate verification statistics on a single ensemble member run for a single machine learning model.

    :param run_date:
    :param member:
    :param window_size:
    :param config:
    :param score_columns:
    :return:
    """
    try:
        print("Starting {0} {1:02d} hours Run: {2}".format(member, window_size, run_date.strftime("%Y%m%d")))
        grid_eval = GridEvaluator(run_date, config.ensemble_name, member, config.model_names, config.size_thresholds,
                                  config.start_hour, config.end_hour, window_size, config.time_skip,
                                  config.forecast_sample_path, config.mrms_path, config.mrms_variable,
                                  config.obs_mask, config.mask_variable)
        print("Loading forecasts {0} {1:02d} hours Run: {2}".format(member, window_size, run_date.strftime("%Y%m%d")))
        grid_eval.load_forecasts()
        print("Window forecasts {0} {1:02d} hours Run: {2}".format(member, window_size, run_date.strftime("%Y%m%d")))
        grid_eval.get_window_forecasts()
        print("Load obs {0} {1:02d} hours Run: {2}".format(member, window_size, run_date.strftime("%Y%m%d")))
        grid_eval.load_obs()
        grid_eval.dilate_obs(config.dilation_radius)
        print("Scoring ROC {0} {1:02d} hours Run: {2}".format(member, window_size, run_date.strftime("%Y%m%d")))
        roc_curves = grid_eval.roc_curves(config.forecast_thresholds)
        print("Scoring Rel {0} {1:02d} hours Run: {2}".format(member, window_size, run_date.strftime("%Y%m%d")))
        rel_curves = grid_eval.reliability_curves(config.forecast_thresholds)
        output_scores = pd.DataFrame(columns=score_columns)
        for ml_model_name in roc_curves.keys():
            for size_threshold in roc_curves[ml_model_name].keys():
                for hour_window in roc_curves[ml_model_name][size_threshold].keys():
                    row = [run_date, config.ensemble_name, member, ml_model_name, size_threshold, window_size,
                           hour_window[0], hour_window[1], roc_curves[ml_model_name][size_threshold][hour_window],
                           rel_curves[ml_model_name][size_threshold][hour_window]]
                    index = "{0}_{1}_{2}_{3}_{4:02d}_{5:02d}_{6:02d}".format(run_date.strftime("%Y%m%d"),
                                                                             config.ensemble_name,
                                                                             member,
                                                                             ml_model_name.replace(" ", "-"),
                                                                             size_threshold,
                                                                             window_size,
                                                                             hour_window[0])
                    print(index)
                    output_scores.loc[index] = row
        return output_scores
    except Exception as e:
        print(traceback.format_exc())
        raise e

if __name__ == "__main__":
    main()
