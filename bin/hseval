#!/usr/bin/env python
import argparse
import traceback
from hagelslag.util.Config import Config
import pandas as pd
import numpy as np
import os
from glob import glob
from multiprocessing import Pool
from hagelslag.evaluation.ObjectEvaluator import ObjectEvaluator
from hagelslag.evaluation.GridEvaluator import GridEvaluator
from hagelslag.evaluation.NeighborEvaluator import NeighborEvaluator


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("config", help="Config file")
    parser.add_argument("-o", "--obj", action="store_true", help="Perform object-based evaluation.")
    parser.add_argument("-g", "--grid", action="store_true", help="Perform grid-based evaluation.")
    parser.add_argument("-n", "--neighbor", action="store_true", help="Perform neighborhood probability evaluation.")
    parser.add_argument("-p", "--proc", type=int, default=1, help="Number of processors.")
    args = parser.parse_args()
    required = ["ensemble_name", "ensemble_members", "start_date", "end_date", "start_hour", "end_hour", "window_sizes",
                "time_skip", "model_names", "model_types", "size_thresholds", "forecast_json_path",
                "track_data_csv_path", "forecast_sample_path", "mrms_path", "mrms_variable", "obs_mask",
                "mask_variable", "forecast_thresholds", "forecast_bins", "out_path", "obj_scores_file",
                "grid_scores_file", "coordinate_file", "lon_bounds", "lat_bounds"]
    config = Config(args.config, required_attributes=required)
    num_procs = args.proc
    if args.obj:
        evaluate_objects(config, num_procs)
        #all_scores.to_csv(config.out_path + config.obj_scores_file, index_label="Index")
    if args.grid:
        evaluate_grids(config, num_procs)
        #grid_scores.to_csv(config.out_path + config.grid_scores_file, index_label="Index")
    if args.neighbor:
        evaluate_neighborhood_probabilities(config, num_procs)
    return


def evaluate_objects(config, num_procs):
    pool = Pool(num_procs)
    run_dates = pd.DatetimeIndex(start=config.start_date,
                                 end=config.end_date, freq="1D")
    ensemble_members = config.ensemble_members
    score_columns = ["Run_Date", "Ensemble_Name", "Ensemble_Member", "Model_Name", "Model_Type", "Forecast_Hour", "CRPS"]
    score_columns += ["ROC_{0:d}".format(int(t)) for t in config.object_thresholds]
    score_columns += ["Rel_{0:d}".format(int(t)) for t in config.object_thresholds]
    score_list = []

    def append_scores(score_set):
        score_list.append(score_set)
        if len(score_list) == 1:
            score_set.to_csv(config.out_path + config.obj_scores_file, index_label="Index")
        else:
            score_set.to_csv(config.out_path + config.obj_scores_file, mode="a", index_label="Index", header=False)

    for run_date in run_dates:
        for member in ensemble_members:
            pool.apply_async(evaluate_object_run, (run_date, member, config, score_columns),
                             callback=append_scores)
    pool.close()
    pool.join()
    return pd.concat(score_list)


def evaluate_object_run(run_date, ensemble_member, config, score_columns):
    try:
        run_id = "{0} {1}".format(run_date, ensemble_member) 
        print("Starting " + run_id)
        object_eval = ObjectEvaluator(run_date, config.ensemble_name, ensemble_member, config.model_names,
                                    config.model_types, config.forecast_bins, config.forecast_json_path,
                                    config.track_data_csv_path)
        print("Loading forecasts " + run_id)
        object_eval.load_forecasts()
        print("Loading obs" + run_id)
        object_eval.load_obs()
        print("Merging obs" + run_id)
        object_eval.merge_obs()
        scores = pd.DataFrame(columns=score_columns)
        query_start = "(Hail_Size>0)"
        forecast_hours = np.arange(config.start_hour, config.end_hour + 1, dtype=int)
        for model_type in config.model_types:
            for model_name in config.model_names:
                for forecast_hour in forecast_hours:
                    index = "{0}_{1}_{2}_{3}_{4}_{5:d}".format(run_date.strftime("%Y%m%d"), config.ensemble_name,
                                                               ensemble_member, model_name, model_type, forecast_hour)
                    print(index)
                    query = query_start + " & " + "(Forecast_Hour == {0:d})".format(forecast_hour)
                    crps = object_eval.crps(model_type, model_name, query=query)
                    rocs = []
                    rels = []
                    for threshold in config.object_thresholds:
                        rocs.append(object_eval.roc(model_type, model_name, threshold,
                                                    config.forecast_thresholds, query=query))
                        rels.append(object_eval.reliability(model_type, model_name, threshold,
                                                            config.forecast_thresholds, query=query))
                    row = [run_date, config.ensemble_name, ensemble_member, model_name, model_type, forecast_hour, crps]
                    row += rocs + rels
                    scores.loc[index] = row
        return scores
    except Exception as e:
        print(traceback.format_exc())
        raise e


def evaluate_grids(config, num_procs):
    pool = Pool(num_procs)
    run_dates = pd.DatetimeIndex(start=config.start_date,
                                 end=config.end_date, freq="1D")
    ensemble_members = config.ensemble_members
    score_columns = ["Run_Date", "Ensemble_Name", "Ensemble_Member", "Model_Name", "Size_Threshold", "Window_Size",
                     "Window_Start", "Window_End", "ROC", "Reliability"]
    score_list = []

    def append_scores(score_set):
        score_list.append(score_set)
        if len(score_list) == 1:
            score_set.to_csv(config.out_path + config.grid_scores_file, index_label="Index")
        else:
            score_set.to_csv(config.out_path + config.grid_scores_file, mode="a", index_label="Index", header=False)
    for window_size in config.window_sizes:
        for run_date in run_dates:
            for member in ensemble_members:
                pool.apply_async(evaluate_grid_run, (run_date, member, window_size, config,
                                                     score_columns),
                                 callback=append_scores)
    pool.close()
    pool.join()
    all_scores = pd.concat(score_list)
    return all_scores


def evaluate_grid_run(run_date, member, window_size, config, score_columns):
    """
    Calculate verification statistics on a single ensemble member run for a single machine learning model.

    :param run_date:
    :param member:
    :param window_size:
    :param config:
    :param score_columns:
    :return:
    """
    try:
        print("Starting {0} {1:02d} hours Run: {2}".format(member, window_size, run_date.strftime("%Y%m%d")))
        grid_eval = GridEvaluator(run_date, config.ensemble_name, member, config.model_names, config.size_thresholds,
                                  config.start_hour, config.end_hour, window_size, config.time_skip,
                                  config.forecast_sample_path, config.mrms_path, config.mrms_variable,
                                  config.obs_mask, config.mask_variable)
        print("Loading forecasts {0} {1:02d} hours Run: {2}".format(member, window_size, run_date.strftime("%Y%m%d")))
        grid_eval.load_forecasts()
        print("Window forecasts {0} {1:02d} hours Run: {2}".format(member, window_size, run_date.strftime("%Y%m%d")))
        grid_eval.get_window_forecasts()
        print("Load obs {0} {1:02d} hours Run: {2}".format(member, window_size, run_date.strftime("%Y%m%d")))
        grid_eval.load_obs()
        grid_eval.dilate_obs(config.dilation_radius)
        print("Scoring ROC {0} {1:02d} hours Run: {2}".format(member, window_size, run_date.strftime("%Y%m%d")))
        roc_curves = grid_eval.roc_curves(config.forecast_thresholds)
        print("Scoring Rel {0} {1:02d} hours Run: {2}".format(member, window_size, run_date.strftime("%Y%m%d")))
        rel_curves = grid_eval.reliability_curves(config.forecast_thresholds)
        output_scores = pd.DataFrame(columns=score_columns)
        for ml_model_name in roc_curves.keys():
            for size_threshold in roc_curves[ml_model_name].keys():
                for hour_window in roc_curves[ml_model_name][size_threshold].keys():
                    row = [run_date, config.ensemble_name, member, ml_model_name, size_threshold, window_size,
                           hour_window[0], hour_window[1], roc_curves[ml_model_name][size_threshold][hour_window],
                           rel_curves[ml_model_name][size_threshold][hour_window]]
                    index = "{0}_{1}_{2}_{3}_{4:02d}_{5:02d}_{6:02d}".format(run_date.strftime("%Y%m%d"),
                                                                             config.ensemble_name,
                                                                             member,
                                                                             ml_model_name.replace(" ", "-"),
                                                                             size_threshold,
                                                                             window_size,
                                                                             hour_window[0])
                    print(index)
                    output_scores.loc[index] = row
        return output_scores
    except Exception as e:
        print(traceback.format_exc())
        raise e


def evaluate_neighborhood_probabilities(config, num_procs):
    """
    Calculate evaluation statistics for a set of neighborhood probability forecasts.

    Parameters
    ----------
    config : hagelslag.util.Config object
        Object containing configuration parameters as attributes.
    num_procs : int
        Number of processors to use for the calculations

    """
    pool = Pool(num_procs)
    run_dates = pd.DatetimeIndex(sorted(os.listdir(config.neighbor_path)))
    
    def save_scores(output):
        hour_scores = output[0]
        period_scores = output[1]
        if not os.access(config.neighbor_score_path + "hour_scores.csv", os.R_OK):
            hour_scores.to_csv(config.neighbor_score_path + "hour_scores.csv", index_label="Index")
            period_scores.to_csv(config.neighbor_score_path + "period_scores.csv", index_label="Index")
        else:
            hour_scores.to_csv(config.neighbor_score_path + "hour_scores.csv", index_label="Index", mode="a",
                               header=False)
            period_scores.to_csv(config.neighbor_score_path + "period_scores.csv", index_label="Index", mode="a",
                                 header=False)
    for run_date in run_dates:
        forecast_files = glob(config.neighbor_path + "{0}/*.nc".format(run_date.strftime("%Y%m%d")))
        for forecast_file in forecast_files:
            file_comps = forecast_file.split("/")[-1].split("_")
            model_name = file_comps[0]
            forecast_variable = "_".join(file_comps[1:file_comps.index("consensus")])
            if forecast_variable in config.neighbor_thresholds.keys():
                pool.apply_async(evaluate_single_neighborhood, (run_date, config.start_hour, config.end_hour,
                                                                model_name, forecast_variable, config.mrms_variable,
                                                                config.neighbor_radii, config.smoothing_radii,
                                                                config.obs_thresholds,
                                                                config.neighbor_thresholds[forecast_variable],
                                                                config.forecast_thresholds,
                                                                config.obs_mask, config.mask_variable,
                                                                config.neighbor_path, config.mrms_path,
                                                                config.coordinate_file, config.lon_bounds,
                                                                config.lat_bounds),
                                 callback=save_scores)
    pool.close()
    pool.join()


def evaluate_single_neighborhood(run_date, start_hour, end_hour, model_name, forecast_variable, mrms_variable,
                                 neighbor_radii, smoothing_radii, obs_thresholds, size_thresholds, probability_levels,
                                 obs_mask, mask_variable, forecast_path, mrms_path, coordinate_file, lon_bounds,
                                 lat_bounds):
    """
    Calculates verification statistics for a single set of model forecasts.

    :param run_date:
    :param start_hour:
    :param end_hour:
    :param model_name:
    :param forecast_variable:
    :param mrms_variable:
    :param neighbor_radii:
    :param smoothing_radii:
    :param size_thresholds:
    :param probability_levels:
    :param obs_mask:
    :param mask_variable:
    :param forecast_path:
    :param mrms_path:
    :return:
    """
    try:
        ne = NeighborEvaluator(run_date, start_hour, end_hour, model_name, forecast_variable, mrms_variable,
                               neighbor_radii, smoothing_radii, obs_thresholds, size_thresholds, probability_levels,
                               obs_mask, mask_variable, forecast_path, mrms_path, coordinate_file, lon_bounds,
                               lat_bounds)
        ne.load_forecasts()
        ne.load_obs()
        ne.load_coordinates()
        hourly_scores = ne.evaluate_hourly_forecasts()
        period_scores = ne.evaluate_period_forecasts()
        return hourly_scores, period_scores
    except Exception as e:
        print(traceback.format_exc())
        raise e


if __name__ == "__main__":
    main()
