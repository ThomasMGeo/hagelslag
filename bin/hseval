#!/usr/bin/env python
import argparse
import traceback
from hagelslag.util.Config import Config
import pandas as pd
from multiprocessing import Pool
from hagelslag.evaluation.ObjectEvaluator import ObjectEvaluator
from hagelslag.evaluation.GridEvaluator import GridEvaluator
import cPickle

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("config", help="Config file")
    parser.add_argument("-o", "--obj", action="store_true", help="Perform object-based evaluation.")
    parser.add_argument("-g", "--grid", action="store_true", help="Perform grid-based evaluation.")
    parser.add_argument("-p", "--proc", type=int, default=1, help="Number of processors.")
    args = parser.parse_args()
    config = Config(args.config, )
    num_procs = args.proc
    if args.obj:
        evaluate_objects(config, num_procs)
    if args.grid:
        grid_scores = evaluate_grids(config, num_procs)
        pickle_file = open(config.out_path + "grid_scores.pkl", "w")
        cPickle.dump(grid_scores, pickle_file, cPickle.HIGHEST_PROTOCOL)
        pickle_file.close()
    return


def evaluate_objects(config, num_procs):
    pool = Pool(num_procs)
    run_dates = pd.DatetimeIndex(start=config.start_date,
                                 end=config.end_date, freq="1D")
    ensemble_members = config.ensemble_members
    score_columns = ["Run_Date", "Ensemble_Name", "Ensemble_Member", "Model_Name", "Model_Type", "CRPS"]
    score_list = []
    for run_date in run_dates:
        for member in ensemble_members:
            pool.apply_async(evaluate_object_run, (run_date, member, config, score_columns),
                             callback=score_list.append)
    pool.close()
    pool.join()
    return pd.concat(score_list)


def evaluate_object_run(run_date, ensemble_member, config, score_columns):
    object_eval = ObjectEvaluator(run_date, config.ensemble_name, ensemble_member, config.model_names,
                                  config.model_types, config.forecast_bins, config.forecast_json_path,
                                  config.track_data_csv_path)
    object_eval.load_forecasts()
    object_eval.load_obs()
    object_eval.merge_obs()

    return


def evaluate_grids(config, num_procs):
    pool = Pool(num_procs)
    run_dates = pd.DatetimeIndex(start=config.start_date,
                                 end=config.end_date, freq="1D")
    ensemble_members = config.ensemble_members
    score_columns = ["Run_Date", "Ensemble_Name", "Ensemble_Member", "Model_Name", "Size_Threshold", "Window_Size",
                     "Window_Start", "Window_End", "ROC", "Reliability"]
    score_list = []
    for run_date in run_dates:
        for member in ensemble_members:
            for window_size in config.window_sizes:
                pool.apply_async(evaluate_grid_run, (run_date, member, window_size, config,
                                                     score_columns),
                                 callback=score_list.append)
    pool.close()
    pool.join()
    all_scores = pd.concat(score_list)
    return all_scores


def evaluate_grid_run(run_date, member, window_size, config, score_columns):
    """
    Calculate verification statistics on a single ensemble member run for a single machine learning model.

    :param run_date:
    :param member:
    :param model_name:
    :param window_size:
    :param config:
    :param score_columns:
    :return:
    """
    try:
        print("Starting {0} {2:02d} hours Run: {3}".format(member, window_size, run_date.strftime("%Y%m%d")))
        grid_eval = GridEvaluator(run_date, config.ensemble_name, member, config.model_names, config.size_thresholds,
                                  config.start_hour, config.end_hour, window_size, config.time_skip,
                                  config.forecast_sample_path, config.mrms_path, config.mrms_variable,
                                  config.obs_mask, config.mask_variable)
        print("Loading forecasts {0} {2:02d} hours Run: {3}".format(member, window_size, run_date.strftime("%Y%m%d")))
        grid_eval.load_forecasts()
        print("Window forecasts {0} {2:02d} hours Run: {3}".format(member, window_size, run_date.strftime("%Y%m%d")))
        grid_eval.get_window_forecasts()
        print("Load obs {0} {2:02d} hours Run: {3}".format(member, window_size, run_date.strftime("%Y%m%d")))
        grid_eval.load_obs()
        print("Scoring {0} {1} {2:02d} hours Run: {3}".format(member, window_size, run_date.strftime("%Y%m%d")))
        roc_curves = grid_eval.roc_curves(config.forecast_thresholds, config.dilation_radius)
        rel_curves = grid_eval.reliability_curves(config.forecast_thresholds, config.dilation_radius)
        output_scores = pd.DataFrame(columns=score_columns)
        for ml_model_name in roc_curves.keys():
            for size_threshold in roc_curves[ml_model_name].keys():
                for hour_window in roc_curves[ml_model_name][size_threshold].keys():
                    row = [run_date, config.ensemble_name, member, ml_model_name, size_threshold, window_size,
                           hour_window[0], hour_window[1], roc_curves[ml_model_name][size_threshold][hour_window],
                           rel_curves[ml_model_name][size_threshold][hour_window]]
                    index = "{0}_{1}_{2}_{3}_{4:02d}_{5:02d}_{6:02d}".format(run_date.strftime("%Y%m%d"),
                                                                             config.ensemble_name,
                                                                             member,
                                                                             ml_model_name.replace(" ", "-"),
                                                                             size_threshold,
                                                                             window_size,
                                                                             hour_window[0])
                    output_scores.loc[index] = row
        return output_scores
    except Exception as e:
        print(traceback.format_exc())
        raise e

if __name__ == "__main__":
    main()
