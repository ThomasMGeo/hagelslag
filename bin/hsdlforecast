#!/usr/bin/env python
from hagelslag.processing.DLPatchSelector import DLPatchSelector
from hagelslag.util.Config import Config
from keras.optimizers import SGD, Adam
from keras.regularizers import l2
import keras.backend as K
from keras import layers
from keras import models
import argparse, pdb
import pandas as pd
import numpy as np
import h5py
import os
np.random.seed(123)  # for reproducibility

def main():
    """
    Main function to parse out configuration file (Config), a dictionary 
    of different model tunnings, for slicing model and observational data. 

    For a given number of parallel processesors, the model and observational 
    data are sliced each day with the model data separated by ensemble member. 
    """
    parser = argparse.ArgumentParser("hsdata - Hagelslag Data Processor")
    parser.add_argument("config", help="Configuration file")
    parser.add_argument("-t", "--train", action="store_true", help="Train machine learning models.")
    parser.add_argument("-f", "--fore", action="store_true", help="Generate forecasts from machine learning models.")
    args = parser.parse_args()
    required = ['start_dates', 'end_dates','start_hour', 'end_hour', 'ensemble_members',
                'model_path', "ensemble_name", 'mrms_path', 'storm_variables', 
                'potential_variables', 'model_map_file','hf_path', 
                'patch_radius','train', 'single_step','num_examples','class_percentages'] 
    #Add attributes of dict to config
    config = Config(args.config, required_attributes=required)
    config.valid_hours = np.arange(config.start_hour, config.end_hour)
    if not hasattr(config, "run_date_format"):
        config.run_date_format = "%Y%m%d-%H"
    config.forecast_variables = config.storm_variables + config.potential_variables
    if hasattr(config, "tendency_variables"):
        config.forecast_variables.extend(config.tendency_variables)
    short_variable_names = []
    for variable in config.forecast_variables:
        if " " in variable: 
            variable_name=''.join([v[0].upper() for v in variable.split()]) + variable.split('_')[-1]
        elif "_" in variable: 
            variable_name= ''.join([v[0].upper() for v in variable.split()]) + variable.split('_')[-1]
        else:
            variable_name = variable
        short_variable_names.append(variable_name)
    
    #Process data for different processor arguments
    dl_patch = DLPatchSelector(config.model_path,config.hf_path,
        config.start_dates,config.end_dates,config.num_examples,
        config.class_percentages,config.patch_radius,
        config.run_date_format,np.array(short_variable_names)) #config.forecast_variables)
    

    for member in config.ensemble_members:
        if args.train:
            mode = 'train' 
            model_data, obs_data  = dl_patch.load_data(member,mode)
            train_models(config,member,model_data,obs_data)
        elif args.fore:
            mode = 'forecast' 
            _ = load_data(config,member,mode)
    return

def train_models(config,member,model_data,model_labels):
    print('\nTraining {0} models'.format(member))
    print('Model data shape {0}'.format(np.shape(model_data)))
    print('Label data shape {0}\n'.format(np.shape(model_labels)))

    scaling_file = config.model_path+'/{0}_{1}_{2}_{3}_training_scaling_values.h5'.format(
        member,config.start_dates['train'].strftime('%Y%m%d'),
        config.end_dates['train'].strftime('%Y%m%d'),config.num_examples)

    #Standardizing data
    if not os.path.exists(scaling_file):
        scaling_values = pd.DataFrame(np.zeros((len(config.forecast_variables), 2), 
            dtype=np.float32),index=config.forecast_variables,
            columns=['mean','std'])

        standard_model_data = np.ones(np.shape(model_data))*np.nan
        for n in range(len(config.forecast_variables)):
            scaling_values['mean'][n] = np.nanmean(model_data[:,:,:,n])
            scaling_values['std'][n] = np.nanstd(model_data[:,:,:,n])
            standard_model_data[:,:,:,n] = (model_data[:,:,:,n]-scaling_values['mean'][n])/scaling_values['std'][n] 
        #Output training scaling values
        output_models(standard_model_data,scaling_file)
        del model_data,scaling_values
    else:
        #Open scaling values file
        standard_model_data = open_models(scaling_file)
        del model_data


    #Initiliaze Convolutional Neural Net (CNN)
    model = models.Sequential()
    l2_a= 0.001

    #First layer, input shape (y,x,# variables) 
    model.add(layers.Conv2D(32, (3, 3), activation='relu', 
        padding="same", kernel_regularizer=l2(l2_a),
        input_shape=(np.shape(standard_model_data[0]))))
    model.add(layers.Dropout(0.1))
    model.add(layers.MaxPooling2D((2, 2))) 
    #Second layer
    model.add(layers.Conv2D(64, (3, 3), activation='relu',padding="same", kernel_regularizer=l2(l2_a)))
    model.add(layers.Dropout(0.1))
    model.add(layers.MaxPooling2D((2, 2)))
    #Third layer
    model.add(layers.Conv2D(64, (3, 3), activation='relu',padding="same", kernel_regularizer=l2(l2_a)))
    model.add(layers.Dropout(0.1))
    model.add(layers.MaxPooling2D((2, 2)))
    
    #Flatten the last convolutional layer into a long feature vector
    model.add(layers.Flatten())
    model.add(layers.Dense(64, activation='relu'))
    model.add(layers.Dense(4, activation='sigmoid'))

    opt = Adam()
    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['acc'])
    
    print(np.shape(standard_model_data),np.shape(model_labels))
    batches = int(config.num_examples/20.0)
    print(batches)
    conv_hist = model.fit(standard_model_data, model_labels, epochs=20, batch_size=batches)
    model.save(
        config.model_path+'/{0}_{1}_{2}_CNN_model.h5'.format(
        member,
        config.start_dates['train'].strftime('%Y%m%d'),
        config.end_dates['train'].strftime('%Y%m%d')))
    
    del model_labels
    
def output_models(in_data,out_filename):
    print('\nWriting to {0}\n'.format(out_filename))
    with h5py.File(out_filename, 'w') as hf:
        hf.create_dataset("data",data=in_data,
        compression='gzip',compression_opts=6)

def open_models(out_filename):
    print('\nOpening {0}\n'.format(out_filename))
    with h5py.File(out_filename, 'r') as hf:
        data = hf['data'][()]
    return data 

def brier_skill_score_keras(obs, preds):
    bs = K.mean((preds - obs) ** 2)
    climo = K.mean((obs - K.mean(obs)) ** 2)
    return 1.0 - (bs/climo)

if __name__ == "__main__":
    __spec__ = None
    main()

