#!/usr/bin/env python
import numpy as np
from hagelslag.processing import extract_storm_patches, label_storm_objects
import pygrib
from multiprocessing import Pool
import argparse
from os.path import join, exists
import pandas as pd
import traceback
from scipy.ndimage import gaussian_filter
import pkg_resources
from netCDF4 import Dataset, date2num


def main():
    grib_data = load_ncar_grib_info()
    parser = argparse.ArgumentParser()
    parser.add_argument("-s", "--start_date", help="Start Run Date")
    parser.add_argument("-e", "--end_date", help="End Run Date")
    parser.add_argument("-t", "--start_hour", type=int, default=1, help="Start Forecast Hour")
    parser.add_argument("-n", "--end_hour", type=int, default=48, help="End Forecast Hour")
    parser.add_argument("-v", "--storm_vars", help="Comma-separated list of storm variable numbers")
    parser.add_argument("-a", "--env_vars", help="Comma-separated list of environment variable numbers")
    parser.add_argument("-i", "--in", help="Path to NCAR Ensemble GRIB2 files.")
    parser.add_argument("-o", "--out", help="Path to output netCDF files.")
    parser.add_argument("-p", '--proc', type=int, default=1, help="Number of processors")
    args = parser.parse_args()
    pool = Pool(args.proc)
    run_dates = pd.DatetimeIndex(start=args.start_date, end=args.end_date, freq="1D")
    members = np.arange(1, 11)
    #for run_date in run_dates:
    #    for member in members:
    #        pool.apply_async(extract_ncar_member_storms, (run_date, member, args.start_hour, args.end_hour))
    pool.close()
    pool.join()
    return


def extract_ncar_member_storms(run_date, member, start_hour, end_hour, label_var, storm_vars, env_vars,
                               data_path, out_path, patch_radius, object_finder_method, min_intensity, max_intensity,
                               min_area, max_area, max_range, increment, smoother_sigma):
    """


    Args:
        run_date:
        member:
        start_hour:
        end_hour:
        label_var:
        storm_vars:
        env_vars:
        data_path:
        out_path:
        patch_radius:
        object_finder_method:
        min_intensity:
        max_intensity:
        min_area:
        max_area:
        max_range:
        increment:
        smoother_sigma:

    Returns:

    """
    try:

        forecast_hours = np.arange(start_hour, end_hour + 1)
        all_storm_patches = []
        all_forecast_hours = []
        all_valid_times = []
        grib_info = load_ncar_grib_info()
        for forecast_hour in forecast_hours:
            ncar_grib_filename = join(data_path, run_date.strftime("%Y%m%d%H"),
                                      "post", "mem_{0:d}".format(member),
                                      "ncar_3km_{0}_mem{1:d}_f{2:03d}.grb2".format(run_date.strftime("%Y%m%d%H"),
                                                                                   member,
                                                                                   forecast_hour))
            ncar_env_filename = join(data_path, run_date.strftime("%Y%m%d%H"),
                                      "post", "mem_{0:d}".format(member),
                                      "ncar_3km_{0}_mem{1:d}_f{2:03d}.grb2".format(run_date.strftime("%Y%m%d%H"),
                                                                                   member,
                                                                                   forecast_hour - 1))
            if exists(ncar_grib_filename):
                ncar_grib_file = pygrib.open(ncar_grib_filename)
                storm_values = ncar_grib_file[label_var].values
                storm_label_grid = label_storm_objects(gaussian_filter(storm_values, smoother_sigma),
                                                       object_finder_method,
                                                       min_intensity, max_intensity,
                                                       min_area=min_area, max_area=max_area, max_range=max_range,
                                                       increment=increment)
                j_grid, i_grid = np.meshgrid(np.arange(storm_values.shape[1]), np.arange(storm_values.shape[0]))
                x_grid = j_grid * 3
                y_grid = i_grid * 3
                lats, lons = ncar_grib_file[label_var].latlons()
                storm_patches = extract_storm_patches(storm_label_grid, storm_values, x_grid, y_grid, [forecast_hour],
                                                      dx=3, dt=1, patch_radius=patch_radius)
                all_valid_times.extend([run_date + pd.Timedelta(hours=forecast_hour)] * len(storm_patches))
                all_forecast_hours.extend([forecast_hour] * len(storm_patches))
                for storm_patch in storm_patches:
                    storm_patch.extract_attribute_array(lats, "latitude")
                    storm_patch.extract_attribute_array(lons, "longitude")

                for storm_var in storm_vars:
                    storm_var_vals = ncar_grib_file[storm_var].values
                    for storm_patch in storm_patches:
                        storm_patch.extract_attribute_array(storm_var_vals, str(storm_var) + "_current")
                ncar_grib_file.close()
                ncar_env_file = pygrib.open(ncar_env_filename)
                for env_var in env_vars:
                    storm_var_vals = ncar_env_file[env_var].values
                    for storm_patch in storm_patches:
                        storm_patch.extract_attribute_array(storm_var_vals, str(env_var) + "_prev")
                ncar_env_file.close()
                all_storm_patches.extend(storm_patches)
            else:
                raise OSError(ncar_grib_filename + " not found")
        out_file_name = out_path + "ncar_ens_storm_patches_{0}.nc".format(run_date.strftime("%Y%m%d%H"))
        out_file = Dataset(out_file_name, mode="w", format="NETCDF4")
        out_file.createDimension("p", size=len(storm_patches))
        out_file.createDimension("y", size=patch_radius * 2)
        out_file.createDimension("x", size=patch_radius * 2)

    except Exception as e:
        print(traceback.format_exc())
        raise e


def load_ncar_grib_info():
    """
    Reads the grib table file output by wgrib2 to describe the NCAR ensemble variables and parses the file into
    more useful variables

    Returns:

    """
    ncar_grib_table_file = pkg_resources.resource_filename(pkg_resources.Requirement.parse("hagelslag"),
                                                           "mapfiles/ncar_grib_table.txt")
    print(ncar_grib_table_file)
    raw_grib_data = pd.read_table(ncar_grib_table_file, sep=":", header=None, index_col=0)
    grib_data = pd.DataFrame(index=raw_grib_data.index, columns=["short_name", "long_name", "units", "level",
                                                                 "var_name"], dtype=str)
    grib_data["short_name"] = raw_grib_data[3].str.split().str[0]
    grib_data["long_name"] = raw_grib_data[3].str.split().str[1:-1].str.join(" ")
    grib_data["units"] = raw_grib_data[3].str.split().str[-1].str.strip("[]")
    grib_data["level"] = raw_grib_data[5]
    grib_data["var_name"] = grib_data["long_name"].str.replace(" ", "_").str.cat(
        grib_data["level"].str.replace(" ", "_"), sep="_").str.lower()
    return grib_data

if __name__ == "__main__":
    main()