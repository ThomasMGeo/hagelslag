{
 "metadata": {
  "name": "",
  "signature": "sha256:4746b5adb5197c5b3b7bd95b82f8508525146f06bad6457d916a013796a669b2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Embed plots in web page. No floating window.\n",
      "%matplotlib inline\n",
      "# svg increases resolution when you zoom in (Ctrl-+); png does not.\n",
      "# Use svg format (scalable vector graphics) for plots in web page, not png\n",
      "%config InlineBackend.figure_formats=['png']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Copied from https://www.xormedia.com/natural-sort-order-with-zero-padding/\n",
      "\n",
      "from sys import maxint\n",
      "import re\n",
      "\n",
      "# optional '-' to support negative numbers\n",
      "_num_re = re.compile(r'-?\\d+')\n",
      "# number of chars in the largest possible int\n",
      "_maxint_digits = len(str(maxint))\n",
      "# format for zero padding positive integers\n",
      "_zero_pad_int_fmt = '{{0:0{0}d}}'.format(_maxint_digits)\n",
      "# / is 0 - 1, so that negative numbers will come before positive\n",
      "_zero_pad_neg_int_fmt = '/{{0:0{0}d}}'.format(_maxint_digits)\n",
      "\n",
      "\n",
      "def _zero_pad(match):\n",
      "    n = int(match.group(0))\n",
      "    # if n is negative, we'll use the negative format and flip the number using\n",
      "    # maxint so that -2 comes before -1, ...\n",
      "    return _zero_pad_int_fmt.format(n) \\\n",
      "        if n > -1 else _zero_pad_neg_int_fmt.format(n + maxint)\n",
      "\n",
      "def zero_pad_numbers(s):\n",
      "    return _num_re.sub(_zero_pad, s)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import datetime as dt\n",
      "from datetime import datetime, timedelta\n",
      "import pandas as pd\n",
      "\n",
      "def spc_hailwind_filename(hailwind,year):\n",
      "    name = '/glade/u/home/ahijevyc/share/'+hailwind+'/'\n",
      "    if hailwind=='torn':\n",
      "        return name + 'Actual_tornadoes.csv'\n",
      "    if year>=2008:\n",
      "        name = name + str(year) + \"_\" + hailwind + \".csv\"\n",
      "    if year >= 2005 and year <= 2007:\n",
      "        name = name + \"2005-2007_\"+hailwind+\".csv\"\n",
      "    return name\n",
      "\n",
      "\n",
      "def filter_rpts(rpts, start, end):\n",
      "    # convert GMT to CST\n",
      "    if any(rpts['tz'] == 9):\n",
      "        rpts['yr_mo_dy_time'][rpts['tz']==9] = rpts['yr_mo_dy_time'] - dt.timedelta(hours=6)\n",
      "        rpts['tz'][rpts['tz']==9] = 3\n",
      "        \n",
      "    if any(rpts['tz'] != 3):\n",
      "        print start, end\n",
      "        print rpts[['om','yr_mo_dy_time','tz']][rpts['tz'] != 3]\n",
      "        print \"WARNING - proceeding with program. Wrote to SPC Mar 22 2017 about fixing these lines\"\n",
      "    \n",
      "    times = rpts['yr_mo_dy_time'] + dt.timedelta(hours=6) # Convert from CST to UTC\n",
      "    correct_date = (times >= start) & (times < end)\n",
      "    return rpts[correct_date]\n",
      "\n",
      "\n",
      "def storm_rpts(type, start, end):\n",
      "    \n",
      "    possible_types = [\"wind\", \"hail\", \"torn\"]\n",
      "    if type not in possible_types:\n",
      "        print \"unknown storm report type:\", type\n",
      "        print \"must be \", possible_types\n",
      "        sys.exit(2)\n",
      "\n",
      "    # csv format described in http://www.spc.noaa.gov/wcm/data/SPC_severe_database_description.pdf\n",
      "    # SPC storm report files downloaded from http://www.spc.noaa.gov/wcm/#data to \n",
      "    # yellowstone: ~ahijevyc/share/ March 2017.\n",
      "    if type == \"wind\":\n",
      "        cols = ['om','yr','mo','dy','date','time','tz', 'st','stf','stn','mag','inj',\n",
      "                'fat','loss','closs','slat','slon','elat','elon','len','wid','ns',\n",
      "                'sn','sg','f1','f2','f3','f4','mt']\n",
      "    if type == \"hail\":\n",
      "        cols = ['om','yr','mo','dy','date','time','tz', 'st','stf','stn','sz','inj',\n",
      "                'fat','loss','closs','slat','slon','elat','elon','len','wid','ns',\n",
      "                'sn','sg','f1','f2','f3','f4','mt']\n",
      "    if type == \"torn\":\n",
      "        cols = ['om','yr','mo','dy','date','time','tz', 'st','stf','stn','f','inj',\n",
      "                'fat','loss','closs','slat','slon','elat','elon','len','wid','ns',\n",
      "                'sn','sg','f1','f2','f3','f4','mt']\n",
      "\n",
      "    rpts_file = spc_hailwind_filename(type,run_date.year)\n",
      "    rpts = pd.read_csv(rpts_file,header=None,names=cols, parse_dates=[['yr','mo','dy','time']],\n",
      "                       infer_datetime_format=True)\n",
      "    \n",
      "    rpts = filter_rpts(rpts, start, end)\n",
      "    return rpts\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/glade/apps/opt/pandas/0.14.0/gnu/4.8.2/lib/python2.7/site-packages/pandas-0.14.0-py2.7-linux-x86_64.egg/pandas/io/excel.py:626: UserWarning: Installed openpyxl is not supported at this time. Use >=1.6.1 and <2.0.0.\n",
        "  .format(openpyxl_compat.start_ver, openpyxl_compat.stop_ver))\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.patches import Polygon, PathPatch, Patch\n",
      "from matplotlib.collections import PatchCollection\n",
      "from mpl_toolkits.basemap import Basemap\n",
      "from glob import glob\n",
      "from itertools import cycle\n",
      "import sys, json, os\n",
      "import numpy as np\n",
      "from hagelslag.data import ModelOutput\n",
      "from hagelslag.processing import read_geojson\n",
      "from hagelslag.util.make_proj_grids import read_ncar_map_file, make_proj_grids\n",
      "from netCDF4 import Dataset\n",
      "from mysavfig import mysavfig\n",
      "plt.rcParams.update({'mathtext.default': 'regular'})\n",
      "\n",
      "def load_json_tracks(path, run_date, member, model_format):\n",
      "    storm_tracks = []\n",
      "    search_str = path + run_date.strftime(\"%Y%m%d/\") + member + \"/\" + model_format + \"*.json\"\n",
      "\n",
      "    model_files = sorted(glob(search_str), key=zero_pad_numbers)\n",
      "    if not model_files:\n",
      "        print \"search str=\",search_str\n",
      "        print \"path=\",path\n",
      "        print \"run_date=\",run_date\n",
      "        print \"member=\",member\n",
      "        print \"model_format=\",model_format\n",
      "\n",
      "    old_mtime = 0\n",
      "    for model_file in model_files:\n",
      "        # Check if modification time is earlier than previous file. \n",
      "        # This indicates an older run still cluttering the directory.\n",
      "        mtime = os.path.getmtime(model_file)\n",
      "        if mtime < old_mtime:\n",
      "            print model_file, mtime, \"was modified before\", old_model_file, old_mtime\n",
      "            print \"an older run may be cluttering \"+search_str\n",
      "            sys.exit(0)\n",
      "        storm_tracks.append(read_geojson(model_file))\n",
      "        old_mtime = mtime\n",
      "        old_model_file = model_file\n",
      "    return storm_tracks\n",
      "\n",
      "def add_basemap(ax,drawcounties=True):\n",
      "    proj_dict, grid_dict = read_ncar_map_file(\"/glade/p/work/ahijevyc/hagelslag/mapfiles/VSE.txt\")\n",
      "    m = Basemap(resolution=\"i\", # \"c\"< \"l\"< \"i\" <\"h\"\n",
      "            llcrnrlon=grid_dict[\"sw_lon\"],\n",
      "            urcrnrlon=grid_dict[\"ne_lon\"],\n",
      "            llcrnrlat=grid_dict[\"sw_lat\"],\n",
      "            urcrnrlat=grid_dict[\"ne_lat\"],\n",
      "            rsphere = (proj_dict[\"a\"], proj_dict[\"b\"]), \n",
      "            projection=proj_dict[\"proj\"], lat_2=proj_dict[\"lat_2\"], lat_1=proj_dict[\"lat_1\"],\n",
      "             lat_0=proj_dict[\"lat_0\"], lon_0=proj_dict[\"lon_0\"], ax=ax)\n",
      "    if drawcounties:\n",
      "        m.drawcounties(linewidth=0.05)\n",
      "    m.drawstates()\n",
      "    m.drawcountries()\n",
      "    m.drawcoastlines(linewidth=0.5)\n",
      "    m.drawparallels(np.arange(0.,81.,2.),labels=[True,False,False,False],linewidth=0.4)\n",
      "    meridians = np.arange(0.,351.,2.)\n",
      "    m.drawmeridians(meridians,labels=[False,False,False,True],linewidth=0.4)\n",
      "    return m\n",
      "\n",
      "def output_netcdf_file(filename, mask_grid, proj_dict, grid_dict):\n",
      "    try:\n",
      "        out_set = Dataset(filename, \"w\", clobber=False) # Set clobber to True only if you want to recreate terrain-anchored object file.\n",
      "    except:\n",
      "        print \"not creating\"\n",
      "        return\n",
      "    print \"creating\", filename\n",
      "    out_set.createDimension(\"y\", mask_grid.shape[0])\n",
      "    out_set.createDimension(\"x\", mask_grid.shape[1])\n",
      "    out_set.set_auto_mask(True)\n",
      "    var = out_set.createVariable(\"count\", 'u8', (\"y\", \"x\"), zlib=True)\n",
      "    var[:] = mask_grid\n",
      "    var.long_name = \"object count\"\n",
      "    for k, v in proj_dict.items():\n",
      "        setattr(out_set, k, v)\n",
      "    for k, v in grid_dict.items():\n",
      "        setattr(out_set, k, v)\n",
      "    out_set.close()\n",
      "    print \"created\",filename\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/glade/apps/opt/scikit-learn/0.15.1/gnu/4.8.2/lib/python2.7/site-packages/sklearn/utils/sparsetools/__init__.py:3: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from ._min_spanning_tree import minimum_spanning_tree\n",
        "/glade/apps/opt/scikit-learn/0.15.1/gnu/4.8.2/lib/python2.7/site-packages/sklearn/utils/sparsetools/_graph_validation.py:5: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from ._graph_tools import csgraph_to_dense, csgraph_from_dense,\\\n",
        "/glade/apps/opt/scikit-learn/0.15.1/gnu/4.8.2/lib/python2.7/site-packages/sklearn/utils/sparsetools/__init__.py:4: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from ._traversal import connected_components\n",
        "/glade/apps/opt/scikit-learn/0.15.1/gnu/4.8.2/lib/python2.7/site-packages/sklearn/utils/extmath.py:20: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from ._logistic_sigmoid import _log_logistic_sigmoid\n",
        "/glade/apps/opt/scikit-learn/0.15.1/gnu/4.8.2/lib/python2.7/site-packages/sklearn/utils/extmath.py:22: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from .sparsefuncs_fast import csr_row_norms\n",
        "/glade/apps/opt/scikit-learn/0.15.1/gnu/4.8.2/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:24: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from ..utils import array2d, arrayfuncs, as_float_array, check_arrays\n",
        "/glade/apps/opt/scikit-learn/0.15.1/gnu/4.8.2/lib/python2.7/site-packages/sklearn/metrics/cluster/supervised.py:18: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from .expected_mutual_info_fast import expected_mutual_information\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/glade/apps/opt/scikit-learn/0.15.1/gnu/4.8.2/lib/python2.7/site-packages/sklearn/metrics/pairwise.py:56: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from .pairwise_fast import _chi2_kernel_fast, _sparse_manhattan\n",
        "/glade/apps/opt/scikit-learn/0.15.1/gnu/4.8.2/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from . import cd_fast\n",
        "/glade/apps/opt/scikit-learn/0.15.1/gnu/4.8.2/lib/python2.7/site-packages/sklearn/linear_model/__init__.py:21: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from .sgd_fast import Hinge, Log, ModifiedHuber, SquaredLoss, Huber\n",
        "/glade/apps/opt/scikit-learn/0.15.1/gnu/4.8.2/lib/python2.7/site-packages/sklearn/svm/base.py:8: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from . import libsvm, liblinear\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/glade/apps/opt/scikit-learn/0.15.1/gnu/4.8.2/lib/python2.7/site-packages/sklearn/svm/base.py:9: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from . import libsvm_sparse\n",
        "/glade/apps/opt/scikit-learn/0.15.1/gnu/4.8.2/lib/python2.7/site-packages/sklearn/utils/random.py:9: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from ._random import sample_without_replacement\n"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "members = [\"1km_on_3km_pbl1\"]\n",
      "#members = [\"3km_pbl1\"]\n",
      "model_format = \"VSE\"\n",
      "ensemble_name = \"VSE\"\n",
      "\n",
      "ncf = Dataset(\"/glade/p/work/ahijevyc/hagelslag/mapfiles/\"+model_format+\"_mask.nc\")\n",
      "landmask = ncf.variables[\"usa_mask\"][:]\n",
      "ncf.close()\n",
      "#ncea -y ttl /glade/scratch/ahijevyc/OBJECT_TRACK/track_data_VSE_json_WSPD10MAX_15/2*/*3km_pbl1/*.object_count.nc terrain_obj_file\n",
      "terrain_obj_file = \"/glade/p/work/ahijevyc/hagelslag/out/u.nc\"\n",
      "ncf = Dataset(terrain_obj_file)\n",
      "nobj = ncf.variables[\"count\"][:]\n",
      "ncf.close()\n",
      "\n",
      "model_path = \"/glade/scratch/ahijevyc/\"+ensemble_name+\"/\"\n",
      "odir = \"/glade/p/work/ahijevyc/hagelslag/out/\"\n",
      "#fields = [\"MAX_UPDRAFT_HELICITY_25\", \"MAX_UPDRAFT_HELICITY_25\"]\n",
      "fields = [\"UP_HELI_MAX03_16\",\"UP_HELI_MAX03_50\"]\n",
      "#fields = [\"UP_HELI_MAX03_16\"]\n",
      "#fields = [\"WSPD10MAX_15\",\"WSPD10MAX_15\"]\n",
      "#field = \"HAIL2D\"\n",
      "#field = \"REFL_1KM_AGL\"\n",
      "pixel_thresh = 1\n",
      "SPC_rpts = True\n",
      "\n",
      "dates = [datetime(2005, 1,13,12),\n",
      "         datetime(2005,12,28,12),\n",
      "         datetime(2006, 1,13,12),\n",
      "         datetime(2007, 1, 4,12),\n",
      "         datetime(2007, 1, 7,12),\n",
      "         datetime(2007, 2,12,12),\n",
      "         datetime(2007, 2,13,12),\n",
      "         datetime(2007, 2,24,12),\n",
      "         datetime(2008, 2,12,12),\n",
      "         datetime(2008, 2,16,12),\n",
      "         datetime(2008, 2,17,12),\n",
      "         datetime(2008, 2,25,12),\n",
      "         datetime(2008,12,9,12),\n",
      "         datetime(2009, 2,18,12),\n",
      "         datetime(2009,12,24,12),\n",
      "         datetime(2010, 1,20,12),\n",
      "         datetime(2010,12,31,12),\n",
      "         datetime(2011, 2,28,12),\n",
      "         datetime(2011,12,22,12),\n",
      "         datetime(2012, 1,22,12),\n",
      "         datetime(2012, 1,25,12)]\n",
      "\n",
      "#dates = [ datetime(2007,2,12,12) ]\n",
      "for run_date in dates:\n",
      "\n",
      "    rows = len(members)+1\n",
      "    fig, ax = plt.subplots(rows, figsize=(8.5,5*rows))\n",
      "    color_list = cycle([\"violet\", \"green\", \"cyan\", \"blue\", \"purple\", \"darkgreen\", \"teal\", \"royalblue\"])\n",
      "\n",
      "    windrpts = storm_rpts(\"wind\", run_date, run_date + dt.timedelta(hours=24))\n",
      "    hailrpts = storm_rpts(\"hail\", run_date, run_date + dt.timedelta(hours=24))\n",
      "    tornrpts = storm_rpts(\"torn\", run_date, run_date + dt.timedelta(hours=24))\n",
      "\n",
      "    wlons, wlats = windrpts['slon'].values, windrpts['slat'].values\n",
      "    hlons, hlats = hailrpts['slon'].values, hailrpts['slat'].values\n",
      "    tlons, tlats = tornrpts['slon'].values, tornrpts['slat'].values\n",
      "\n",
      "    m = add_basemap(ax[-1])\n",
      "    final_legend_list = []\n",
      "    iax = -1\n",
      "    \n",
      "    for member, field in zip(members,fields):\n",
      "        json_path = \"/glade/scratch/ahijevyc/OBJECT_TRACK/track_data_\"+ensemble_name+\"_json_\"+field+\"/\"\n",
      "\n",
      "        terraintoo, watertoo = True, True # plot objects over water too\n",
      "        if \"WSPD10MAX\" in field:\n",
      "            watertoo = False # don't plot wind objects over water.\n",
      "            terraintoo = False # don't plot wind objects anchored to terrain.\n",
      "\n",
      "        iax = iax+1\n",
      "        m = add_basemap(ax[iax])\n",
      "\n",
      "        centroids = []\n",
      "        object_sizes = []\n",
      "        model_grid = ModelOutput(ensemble_name,\n",
      "                                 member, run_date, field, run_date, \n",
      "                                 run_date+timedelta(hours=24),\n",
      "                                 model_path,\n",
      "                                 single_step=True)\n",
      "\n",
      "        model_map_file=\"/glade/p/work/ahijevyc/hagelslag/mapfiles/VSE.txt\"\n",
      "        model_grid.load_map_info(model_map_file)\n",
      "        model_grid.data = []\n",
      "\n",
      "        # Initialze overlay_grid with zeros - counts number of times a points is occupied by an object\n",
      "        # over the couse of a model run. One netCDF file is produced for each date and each member.\n",
      "        # Later the output netCDF files can be merged to identify points with objects most \n",
      "        # frequently.  These are likely objects anchored to terrain.\n",
      "        nobj_grid = np.zeros(model_grid.lat.shape, dtype=int)\n",
      "        nobj_nc = json_path + run_date.strftime(\"%Y%m%d/\") + member + run_date.strftime('/%Y%m%d%H.')+\"object_count.nc\"\n",
      "\n",
      "        storm_tracks = load_json_tracks(json_path, run_date, member, model_format)\n",
      "        if not storm_tracks:\n",
      "            # Used to indicate a bug, but this happens legitimately if no objects are found (e.g. 20080225 no 0-3km UH>25 m2/s2)\n",
      "            print \"found no storm tracks for json_path=\",json_path\n",
      "            print \"run_date=\",run_date\n",
      "            print \"member=\",member\n",
      "            print \"model_format=\",model_format\n",
      "        patches = []\n",
      "        for track in storm_tracks:\n",
      "            previous_centroid = None\n",
      "            for time, mask, i, j in zip(track.times, track.masks, track.i, track.j):\n",
      "                osize = track.size(time)\n",
      "                if osize < pixel_thresh:\n",
      "                    print 'skipping small object, %d pixels'% osize\n",
      "                    continue\n",
      "\n",
      "                polygon_xy = track.boundary_polygon(time)\n",
      "                \n",
      "                # Get ij indices covered by object and increment nobj_grid by 1.\n",
      "                ni = (mask * i)[np.nonzero(mask)]\n",
      "                nj = (mask * j)[np.nonzero(mask)]\n",
      "                nobj_grid[ni,nj] = nobj_grid[ni,nj] + 1\n",
      "\n",
      "                if not watertoo and np.mean(landmask[ni,nj]) < 0.5:\n",
      "                    tmp = track.center_of_mass(time)\n",
      "                    print \"object >50% over water. ignoring\", model_grid.proj(*tmp, inverse=True)\n",
      "                    continue\n",
      "                average_obj_frequency = np.mean(nobj[ni,nj])\n",
      "                if not terraintoo and average_obj_frequency > 50:\n",
      "                    tmp = track.center_of_mass(time)\n",
      "                    print \"average obj frequency\", average_obj_frequency\n",
      "                    print \"may be anchored to terrain. ignoring\", model_grid.proj(*tmp, inverse=True)\n",
      "                    continue\n",
      "                    \n",
      "                object_sizes.append(osize)\n",
      "                centroid = track.center_of_mass(time)\n",
      "                # convert centroid from xkm ykm to lon lat\n",
      "                centroid = model_grid.proj(*centroid, inverse=True)\n",
      "                centroids.append(centroid)\n",
      "\n",
      "                # expand polygon_lonlat into two arguments (asterisk)\n",
      "                # Feed to m() map transformation\n",
      "                # Transpose 2xN array to Nx2. \n",
      "                # Sometimes boundary_polygon returns 2 empty arrays (4 pixels?). Don't plot it if that is the case\n",
      "                if polygon_xy[0].size > 0:\n",
      "                    # use these values in xkm and ykm.\n",
      "                    # Use model_grid.proj object to go to lonlat\n",
      "                    polygon_lonlat = model_grid.proj(*polygon_xy, inverse=True)\n",
      "                    patches.append(Polygon(np.transpose(m(*polygon_lonlat)), closed=True, fill=True))\n",
      "                else:\n",
      "                    print \"no boundary polygon. size\", track.size(time), \"mask\", mask\n",
      "\n",
      "                # label storm objects with ho ur\n",
      "                ax[iax].annotate(str(time), xy=m(*centroid), fontsize=5.2)\n",
      "                # Draw line from previous object to this one.\n",
      "                if previous_centroid is not None:\n",
      "                    m.drawgreatcircle(previous_centroid[0], previous_centroid[1], centroid[0], centroid[1],color=\"black\")\n",
      "                previous_centroid = centroid\n",
      "\n",
      "        output_netcdf_file(nobj_nc,nobj_grid,{},{})\n",
      "        \n",
      "        area = np.sum(object_sizes)*(model_grid.dx/1000)**2\n",
      "        color = next(color_list)\n",
      "        ax[iax].set_title(member + \" \" + run_date.strftime(\"%Y%m%d%H\")+ \" \" + str(area) + \" km\" + r'$^2$' +\"\\n\"+\n",
      "                          field+ r'$ \\geq$' +\"%d pixels\"%pixel_thresh + \" watertoo: \" + str(watertoo) +\n",
      "                          \" terraintoo: \" + str(terraintoo))\n",
      "        alpha = 0.38\n",
      "        pc = PatchCollection(patches, color=color, alpha=alpha)\n",
      "        ax[iax].add_collection(pc) # Plot individual member\n",
      "\n",
      "        ax[iax].legend(handles=[Patch(color=color,alpha=alpha,label=member)],loc=\"upper left\",numpoints=1,fontsize=9)\n",
      "        # For some reason, if I substitute the literal PatchCollection instance below with \n",
      "        # the variable \"pc\", no patches appear in any of the subplots.\n",
      "        # Tack on individual member to final, composite plot.\n",
      "        # First member is opaque; successive members are more transparent\n",
      "        alpha = 1-np.sqrt(float(iax)/len(members))\n",
      "        ax[-1].add_collection(PatchCollection(patches, color=color, alpha=alpha))\n",
      "        final_legend_list.append(Patch(color=color,alpha=alpha,label=member))\n",
      "\n",
      "        if len(centroids) > 0:\n",
      "            # Get area-weighted centroid for all objects\n",
      "            overall_centroid = np.average(centroids, weights = object_sizes, axis=0)\n",
      "            m.plot(*overall_centroid, marker='x', color=color, markersize=40, latlon=True)\n",
      "            centroid_on_final_panel, = m.plot(*overall_centroid, marker='x', color=color, \n",
      "                                              label = member + ' centroid', markersize=40, latlon=True, \n",
      "                                              linestyle='None', ax=ax[-1])\n",
      "            final_legend_list.append(centroid_on_final_panel)\n",
      "            \n",
      "    \n",
      "    ax[-1].set_title(','.join(members)+ \" \" + run_date.strftime(\"%Y%m%d%H\")+\"\\n\"+'_'.join(fields)+ r'$ \\geq$'+ \"%d pixels\"%pixel_thresh)\n",
      "\n",
      "    norpts_str = \".norpts\"\n",
      "    if SPC_rpts:\n",
      "        wrpts, = m.plot(wlons, wlats, 'bo', markersize=2.5, marker=\"s\", latlon=True, label=\"Wind Report\", alpha=0.5, ax=ax[-1])\n",
      "        hrpts, = m.plot(hlons, hlats, 'go', markersize=3, marker=\"^\", latlon=True, label=\"Hail Report\", alpha=0.5, ax=ax[-1])\n",
      "        trpts, = m.plot(tlons, tlats, 'ro', markersize=3, marker=\"v\", latlon=True, label=\"Tornado Rpt\", alpha=0.5, ax=ax[-1])\n",
      "        final_legend_list.extend([wrpts, hrpts, trpts])\n",
      "        norpts_str = \"\"\n",
      "\n",
      "\n",
      "    ax[-1].legend(handles=final_legend_list,loc=\"upper left\",numpoints=1,fontsize=7.,framealpha=0.5)\n",
      "    ofile = odir + '_'.join(members) + \"_\" + run_date.strftime(\"%Y%m%d%H_\") + '_'.join(fields) + norpts_str + \".png\" # multi-row plot: 1 per member, plus 1 all-member composite.\n",
      "    ret = mysavfig(ofile)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2005-01-13 12:00:00 2005-01-14 12:00:00\n",
        "        om       yr_mo_dy_time  tz\n",
        "2507   245 1956-06-01 11:33:00   0\n",
        "2855    89 1957-04-02 23:45:00   0\n",
        "8145   216 1965-05-05 14:45:00   6\n",
        "9569   158 1967-04-21 12:33:00   0\n",
        "13409  264 1972-05-13 18:08:00   0\n",
        "15792  804 1974-08-13 15:03:00   0\n",
        "20640  458 1980-06-04 16:30:00   0\n",
        "22101  271 1982-05-11 14:25:00   0\n",
        "24007  200 1984-04-26 19:32:00   0\n",
        "25899  501 1986-07-01 22:15:00   6\n",
        "26054  656 1986-09-04 18:55:00   6\n",
        "28793  419 1990-05-24 15:00:00   6\n",
        "28797  422 1990-05-24 16:00:00   6\n",
        "28810  433 1990-05-24 18:33:00   6\n",
        "29192  815 1990-06-27 20:00:00   6\n",
        "29232  855 1990-07-05 21:10:00   6\n",
        "29347  971 1990-08-15 18:30:00   6\n",
        "33262  151 1994-04-22 18:06:00   6\n",
        "33557  446 1994-05-31 15:00:00   6\n",
        "33572  461 1994-06-06 14:40:00   6\n",
        "33574  463 1994-06-06 15:00:00   6\n",
        "33585  474 1994-06-07 14:47:00   6\n",
        "33586  476 1994-06-07 15:57:00   6\n",
        "33587  477 1994-06-07 16:10:00   6\n",
        "33589  478 1994-06-07 16:35:00   6\n",
        "33592  482 1994-06-07 18:50:00   6\n",
        "33783  672 1994-06-29 15:45:00   6\n",
        "33834  722 1994-07-06 18:17:00   6\n",
        "33855  744 1994-07-12 14:30:00   6\n",
        "33886  775 1994-07-18 15:30:00   6\n",
        "33887  776 1994-07-18 16:00:00   6\n",
        "33888  777 1994-07-18 16:25:00   6\n",
        "33889  778 1994-07-18 16:40:00   6\n",
        "33890  779 1994-07-18 16:55:00   6\n",
        "33891  780 1994-07-18 16:55:00   6\n",
        "33892  781 1994-07-18 17:00:00   6\n",
        "WARNING - proceeding with program. Wrote to SPC Mar 22 2017 about fixing these lines\n",
        "search str="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " /glade/scratch/ahijevyc/OBJECT_TRACK/track_data_VSE_json_UP_HELI_MAX03_16/20050113/1km_on_3km_pbl1/VSE*.json\n",
        "path= /glade/scratch/ahijevyc/OBJECT_TRACK/track_data_VSE_json_UP_HELI_MAX03_16/\n",
        "run_date= 2005-01-13 12:00:00\n",
        "member= 1km_on_3km_pbl1\n",
        "model_format= VSE\n",
        "found no storm tracks for json_path= /glade/scratch/ahijevyc/OBJECT_TRACK/track_data_VSE_json_UP_HELI_MAX03_16/\n",
        "run_date= 2005-01-13 12:00:00\n",
        "member= 1km_on_3km_pbl1\n",
        "model_format= VSE\n",
        "not creating\n",
        "created /glade/p/work/ahijevyc/hagelslag/out/1km_on_3km_pbl1_2005011312_UP_HELI_MAX03_16_UP_HELI_MAX03_50.png dpi=125"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(rows, figsize=(8.5,5*rows))\n",
      "m = add_basemap(ax[-1])\n",
      "m.contourf(model_grid.lon,model_grid.lat,np.ma.array(nobj,mask=nobj<1), cmap=\"OrRd\",latlon=True, vmax=50)\n",
      "ax[-1].set_title(','.join(members)+ \" \" + run_date.strftime(\"%Y%m%d%H\")+\"\\n\"+'_'.join(fields)+ r'$ \\geq$'+ \"%d pixels\"%pixel_thresh)\n",
      "\n",
      "\n",
      "mysavfig(\"t.png\")\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print centroids, object_sizes, np.array(centroids).shape\n",
      "print np.average(centroids, weights = object_sizes, axis=0)\n",
      "tmp = object_sizes\n",
      "tmp[0] = 9999999999\n",
      "print np.average(centroids, weights = object_sizes, axis=0)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dir(track)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}